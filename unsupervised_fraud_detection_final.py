# -*- coding: utf-8 -*-
"""Unsupervised_Fraud_Detection_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cYN4C6BTlCKvaj6NeQVy7BDGdwNFki2r
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE

from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier

!pip install catboost
from catboost import CatBoostClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import IsolationForest
from xgboost import XGBClassifier
from sklearn.svm import OneClassSVM
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score

from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from tensorflow import keras
# deep learning
from keras.models import Model, Sequential
from keras import regularizers
from keras.layers import (
    Input,
    Conv1D,
    MaxPooling1D,
    Dense,
    Flatten,
    Dropout,
    BatchNormalization,
)
from keras.optimizers import Adam

data = pd.read_excel("/content/Customer Transactions Data - Tshidiso.xlsx")

data.head()

data.tail()

data.columns

data.shape

data.info()

data.describe(include = 'all')

corr = data.corr()
corr

data.isnull().sum().any()

data.duplicated().sum()

duplicate = data[data.duplicated()]
duplicate

df1=data.drop_duplicates()

df1.duplicated().sum().any()

df1

df1['Year']=df1[ "CreatedOn"].dt.year

df1['Month']=df1[ "CreatedOn"].dt.month

df1['Day']=df1[ "CreatedOn"].dt.day

df1['Times']=df1[ "CreatedOn"].dt.time
df1['Times'] = df1['Times'].astype(str)

df = df1.drop('CreatedOn', axis=1)
df

label_encoder = preprocessing.LabelEncoder()
df['TrxType']= label_encoder.fit_transform(df['TrxType'])

label_encoder = preprocessing.LabelEncoder()
df['Branch']= label_encoder.fit_transform(df['Branch'])

import pandas as pd
from sklearn.ensemble import IsolationForest
df['Times'] = pd.to_datetime(df['Times'])

df['hour'] = df['Times'].dt.hour
df['minute'] = df['Times'].dt.minute
df['second'] = df['Times'].dt.second

df['Time'] = df['hour'] * 3600 + df['minute'] * 60 + df['second']

df = df.drop(['Times', 'hour', 'minute', 'second'], axis=1)
df.head()

pd.DataFrame(df.nunique(), columns = ["Number of unique values"])

plt.figure(figsize = (25, 20))
plotnumber = 1

for col in df.columns:
    if plotnumber <= 24:
        ax = plt.subplot(5, 5, plotnumber)
        sns.distplot(df[col])
        plt.xlabel(col, fontsize = 15)

    plotnumber += 1

plt.tight_layout()
plt.show()

#Outliers Detection
plt.figure(figsize = (20, 15))
plotnumber = 1

for col in df.columns:
    if plotnumber <= 24:
        ax = plt.subplot(5, 5, plotnumber)
        sns.boxplot(df[col])
        plt.xlabel(col, fontsize = 15)

    plotnumber += 1
plt.tight_layout()
plt.show()

"""K-MEANS & PCA CLUSTERING"""

x = df.iloc[:, [3, 4]].values

columns_to_drop = ["Year", "Month", "Day", "Time"]
df_processed = df.drop(columns=columns_to_drop)

# Apply PCA to reduce the processed data to 3 principal components
pca = PCA(n_components=3)
X_pca = pca.fit_transform(df_processed.values)

n_clusters = 3

kmeans = KMeans(n_clusters= 3, random_state=42)

kmeans.fit(X_pca)

labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Calculate distances of each point to its assigned cluster's centroid
distances = np.zeros_like(labels, dtype=float)
for i in range(n_clusters):
    cluster_points = X_pca[labels == i]
    distances[labels == i] = np.linalg.norm(cluster_points - centroids[i], axis=1)

# Use interquartile range (IQR) method to identify outliers
q1 = np.percentile(distances, 25)
q3 = np.percentile(distances, 75)
iqr = q3 - q1
threshold = q3 + 1.5 * iqr

# Create a binary label where 1 is an outlier and 0 is not an outlier
binary_labels = np.where(distances > threshold, 1, 0)

# Add the binary labels as a new column in the original DataFrame 'df'
df['Class'] = binary_labels


# Create a 3D plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='viridis', alpha=0.7, edgecolors='k')
centroid_scatter = ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c='red', marker='X', s=200, label='Centroids')

ax.set_xlabel('Centriods')
ax.set_title('Suspicious Fraud Transactions Based on Outliers')

plt.show()

df.head()

df.tail()

df1.groupby('Branch')['Amount'].sum().sort_values(ascending=False).plot(kind='bar', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right']].set_visible(False)
plt.ylabel('Total Transaction Amount')
plt.xlabel('Branch')
plt.title('Ranking of Branches by Total Transaction Amount')
plt.show()

binary_labels_counts = df['Class'].value_counts()

print("Sum of Suspicious_Fraud for 1:", binary_labels_counts.get(1, 0))
print("Sum of Non_Fraud for 0:", binary_labels_counts.get(0, 0))

ax = sns.countplot(x='Class',data=df, palette="RdYlBu_r")
for p in ax.patches:
     ax.annotate(f'\n{p.get_height()}', (p.get_x()+0.4, p.get_height()+100), ha='center', va='top', color='white', size=10)

plt.show()

# checking for correlations
corr_matrix = df.corr('spearman')
sns.heatmap(corr_matrix, cbar=True, annot=True, mask = np.triu(np.ones_like(corr_matrix, dtype = bool)), fmt='.3f', cmap='PuBu')
plt.title('Correlation')

df["Class"].value_counts().plot.pie(explode=[0.2, 0.2], autopct='%1.1f%%', shadow=True, colors=['blue', 'red'])
plt.title('Suspicious Fraudulent Transactions')
plt.show()

# setting up separate dataframes for suspicious fraud and non suspicious fraud data analysis and comparison
df4 = df[df["Class"] == 1].reset_index(drop=True)
df5 = df[df["Class"] == 0].reset_index(drop=True)
print("Suspicious Fraud Shape: ", df4.shape)
print("Non-Suspicious Fraud Shape: ", df5.shape)

df4.head()

df5.head()

# Amount distribution comparison

fig = plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(df4["Amount"], color="red")
plt.title("Suspicious Fraud Amount Distribution")
plt.xlabel("Frequency")
plt.ylabel("Amount")

plt.subplot(1, 2, 2)
plt.plot(df5["Amount"], color="blue")
plt.title("Non-Suspicious Fraud Amount Distribution")
plt.xlabel("Frequency")
plt.ylabel("Amount")

plt.show()

# Amount distribution comparison (histogram)
fig = plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.hist(df4["Amount"], color="red")
plt.title("Suspicious Fraud Amount Distribution")
plt.xlabel("Amount")
plt.ylabel("Frequency")

plt.subplot(1, 2, 2)
plt.hist(df5["Amount"], color="blue")
plt.title("Non-Suspicious Fraud Amount Distribution")
plt.xlabel("Amount")
plt.ylabel("Frequency")

plt.show()

# Time vs. Amount distribution comparison (scatter plot)

fig = plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.scatter(df4["Time"], df4["Amount"], color="red")
plt.title("Suspicious Fraud Amount Distribution")
plt.xlabel("Time")
plt.ylabel("Amount")

plt.subplot(1, 2, 2)
plt.scatter(df5["Time"], df5["Amount"], color="blue")
plt.title("Non-Suspicious Fraud Amount Distribution")
plt.xlabel("Time")
plt.ylabel("Amount")

plt.show()

"""**Unsupervised Machine Learning for Anomaly Detection**"""

# Since the dataset is too large, taking a sample from that data to train models
sampled_data = df.sample(frac=0.1, random_state=42)

print("Sampled Fraud Data Shape:", sampled_data.shape)
print("Target Variable Value Count: ")
print(sampled_data["Class"].value_counts())

# splitting data into X and y

X = sampled_data.drop("Class", axis=1)
y = sampled_data["Class"]

print("X Shape: ", X.shape)
print("y Shape: ", y.shape)
print("                                                        ")
print("Target Variable Value Count: ")
print("Normal Class: ", (y == 0).sum())
print("Fraud Class: ", (y == 1).sum())

Suspicious_fraud = sampled_data[sampled_data["Class"] == 1]
non_Suspicious_fraud = sampled_data[sampled_data["Class"] == 0]

outlier_fraction = len(Suspicious_fraud) / float(len(non_Suspicious_fraud))
print("                                                   ")
print("Suspicious Fraud Fraction: ", outlier_fraction)

model = RandomForestClassifier()
rfe = RFE(model, n_features_to_select=1)
rfe.fit(X, y)
feature_ranking = rfe.ranking_

# Output the feature rankings
print("Feature Rankings:")
for feature, rank in zip(X.columns, feature_ranking):
    print(f"{feature}: {rank}")

feature_ranking_dict = dict(zip(X.columns, feature_ranking))
print("\nFeature Rankings (as dictionary):")
print(feature_ranking_dict)

"""## Isolation Forest (Outlier Detection)"""

from sklearn.metrics import (confusion_matrix, roc_curve, classification_report, precision_score, recall_score, accuracy_score, f1_score, roc_auc_score)

clf = IsolationForest(n_estimators=100, max_samples=len(X), random_state=42, contamination=outlier_fraction)

clf.fit(X)
y_pred = clf.predict(X)

print("Predicted Anomalous Data Points: ", len(y_pred[y_pred == -1]))
print("Predicted Non Anomalous Data Points: ", len(y_pred[y_pred == 1]))
print("                                                        ")

# replacing -1 by 1 and 1 by 0
y_pred[y_pred == 1] = 0
y_pred[y_pred == -1] = 1

# evaluation
print("Number of misclassified data points: ", (y_pred != y).sum())
print("\nAccuracy Score :", accuracy_score(y, y_pred))

print("\nClassification Report: ")
print(classification_report(y, y_pred))


print("Model Precision:", round(precision_score(y_pred , y),2))
print("Model Recall:", round(recall_score(y_pred , y),2))
print("Model F1-Score:", round(f1_score(y_pred , y),2))
print("Model ROC:", round(roc_auc_score(y_pred , y),2) , '\n')


conf_matrix=confusion_matrix(y_pred,y)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_pred , y) , '\n')

plt.title("Isolation Forest Classifier - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

# adding the predicted y to the sampled dataset
sampled_data["y_pred"] = y_pred

print(sampled_data["y_pred"].value_counts())

# plotting normal and fraud data points

fig = plt.figure(figsize=(14, 5))

# Isolation Forest Plot
plt.subplot(1, 2, 1)
anomaly_df = sampled_data.loc[sampled_data["y_pred"] == 1, ["Time", "Amount"]]
plt.plot(sampled_data["Time"], sampled_data["Amount"], color="blue")
plt.scatter(anomaly_df["Time"], anomaly_df["Amount"], color="red")
plt.title("Anomalies Detected Using Isolation Forest")
plt.xlabel("Time")
plt.ylabel("Amount")

# Original data plot
plt.subplot(1, 2, 2)
anomaly_df = sampled_data.loc[sampled_data["Class"] == 1, ["Time", "Amount"]]
plt.plot(sampled_data["Time"], sampled_data["Amount"], color="green")
plt.scatter(anomaly_df["Time"], anomaly_df["Amount"], color="red")
plt.title("Original Anomalies")
plt.xlabel("Time")
plt.ylabel("Amount")

plt.show()

"""## Local Outlier Factor (Outlier Detection)"""

from sklearn.neighbors import LocalOutlierFactor

lof = LocalOutlierFactor(
    n_neighbors=20,
    algorithm="auto",
    leaf_size=30,
    metric="minkowski",
    p=2,
    metric_params=None,
    contamination=outlier_fraction,
)

y_pred_lof = lof.fit_predict(X)

print("Predicted Anomalous Data Points: ", len(y_pred_lof[y_pred_lof == -1]))
print("Predicted Non Anomalous Data Points: ", len(y_pred_lof[y_pred_lof == 1]))
print("                                                                ")

# replacing -1 by 1 and 1 by 0
y_pred_lof[y_pred_lof == 1] = 0
y_pred_lof[y_pred_lof == -1] = 1

# evaluation
print("Number of misclassified data points: ", (y_pred_lof != y).sum())
print("\nAccuracy Score :", accuracy_score(y, y_pred_lof))

print("\nClassification Report: ")
print(classification_report(y, y_pred_lof))

print("Model Precision:", round(precision_score(y_pred_lof , y),2))
print("Model Recall:", round(recall_score(y_pred_lof , y),2))
print("Model F1-Score:", round(f1_score(y_pred_lof , y),2))
print("Model ROC:", round(roc_auc_score(y_pred_lof , y),2) , '\n')


conf_matrix=confusion_matrix(y_pred_lof,y)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_pred_lof , y) , '\n')

plt.title("LocalOutlierFactor - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

# adding the predicted y to the sampled dataset
sampled_data["y_pred_lof"] = y_pred_lof
print(sampled_data["y_pred_lof"].value_counts())

# plotting normal and fraud data points

fig = plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
anomaly_df = sampled_data.loc[sampled_data["y_pred_lof"] == 1, ["Time", "Amount"]]
plt.plot(sampled_data["Time"], sampled_data["Amount"], color="blue")
plt.scatter(anomaly_df["Time"], anomaly_df["Amount"], color="red")
plt.title("Anomalies Detected Using LOF")
plt.xlabel("Time")
plt.ylabel("Amount")

plt.subplot(1, 2, 2)
anomaly_df = sampled_data.loc[sampled_data["Class"] == 1, ["Time", "Amount"]]
plt.plot(sampled_data["Time"], sampled_data["Amount"], color="green")
plt.scatter(anomaly_df["Time"], anomaly_df["Amount"], color="red")
plt.title("Original Anomalies")
plt.xlabel("Time")
plt.ylabel("Amount")

plt.show()

"""**Handling Imbalanced Dataset**"""

# creating a smaller sample for handling imbalanced dataset.

df6 = df[df["Class"] == 0].sample(2337)
df7 = df[df["Class"] == 1]
dl_sample_data = df6.append(df7).reset_index(drop=True)

print("Target Variable Value Counts: ")
print(dl_sample_data["Class"].value_counts())

dl_sample_data["Class"].value_counts().plot.pie(explode=[0.2, 0.2], autopct='%1.1f%%', shadow=True, colors=['blue', 'red'])
plt.title('Suspicious Fraudulent Transactions')
plt.show()

# splitting the data

X = dl_sample_data.drop("Class", axis=1)
y = dl_sample_data["Class"]

print("X Shape: ", X.shape)
print("y Shape: ", y.shape)

"""**Visualize the data with t-SNE**"""

def dimensionality_plot(X, y):

    #     sns.set(style='whitegrid', palette='muted')

    # Initializing TSNE object with 2 principal components
    tsne = TSNE(n_components=2, random_state=42)

    # Fitting the data
    X_trans = tsne.fit_transform(X)

    plt.figure(figsize=(10, 6))

    plt.scatter(
        X_trans[np.where(y == 0), 0],
        X_trans[np.where(y == 0), 1],
        marker="o",
        color="b",
        linewidth=1,
        alpha=0.8,
        label="Non Suspicious Fraud",
    )

    plt.scatter(
        X_trans[np.where(y == 1), 0],
        X_trans[np.where(y == 1), 1],
        marker="o",
        color="r",
        linewidth=1,
        alpha=0.8,
        label="Suspicious Fraud",
    )

    plt.legend(loc="best")

    plt.show()

# plotting the dimensionality_plot for original data

dimensionality_plot(X, y)

"""**DEEP LEARNING TECHNIQUES**"""

# scaling the data

scaler = StandardScaler().fit_transform(X)

# scaled data
X_scaled_normal = scaler[y == 0]
X_scaled_fraud = scaler[y == 1]

# setting up the input shape (equal to number of features)
input_layer = Input(shape=(X.shape[1],))

# building the encoder network
encoded = Dense(100, activation="tanh", activity_regularizer=regularizers.l1(10e-5))(
    input_layer
)
encoded = Dense(50, activation="tanh", activity_regularizer=regularizers.l1(10e-5))(
    encoded
)
encoded = Dense(25, activation="tanh", activity_regularizer=regularizers.l1(10e-5))(
    encoded
)
encoded = Dense(12, activation="tanh", activity_regularizer=regularizers.l1(10e-5))(
    encoded
)
encoded = Dense(6, activation="relu")(encoded)

# building the decoder network
decoded = Dense(12, activation="tanh")(encoded)
decoded = Dense(25, activation="tanh")(decoded)
decoded = Dense(50, activation="tanh")(decoded)
decoded = Dense(100, activation="tanh")(decoded)

output_layer = Dense(X.shape[1], activation="sigmoid")(decoded)

# creating the model
autoencoder = Model(input_layer, output_layer)

autoencoder.summary()

# compiling the auto encoder model
autoencoder.compile(optimizer="adam", loss="mse")

history = autoencoder.fit(
    X_scaled_normal,
    X_scaled_normal,
    batch_size=32,
    epochs=50,
    shuffle=True,
    validation_split=0.20,
)

# Loss curve

plt.figure(figsize=[6, 4])
plt.plot(history.history["loss"], "blue", linewidth=2.0)
plt.plot(history.history["val_loss"], "red", linewidth=2.0)
plt.legend(["Training Loss", "Validation Loss"], fontsize=14)
plt.xlabel("Epochs", fontsize=10)
plt.ylabel("Loss", fontsize=10)
plt.title("Loss Curves", fontsize=12)

# creating the sequential model and adding the trainined weights till the fifth layer (till where the latent representation exists)
latent_model = Sequential()
latent_model.add(autoencoder.layers[0])
latent_model.add(autoencoder.layers[1])
latent_model.add(autoencoder.layers[2])
latent_model.add(autoencoder.layers[3])
latent_model.add(autoencoder.layers[4])

# generating the hidden representation of the 2 classes

normal_datapoints = latent_model.predict(X_scaled_normal)
fraud_datapoints = latent_model.predict(X_scaled_fraud)

# creating a dataframe of latent representation of the data
encoded_X = np.append(normal_datapoints, fraud_datapoints, axis=0)

y_normal = np.zeros(normal_datapoints.shape[0])
y_fraud = np.ones(fraud_datapoints.shape[0])
encoded_y = np.append(y_normal, y_fraud, axis=0)

# plotting the TSNE function again to visualize the latent data
dimensionality_plot(encoded_X, encoded_y)

"""**Supervised Machine Learning Techniques for Anomaly Detection**"""

# splitting the data into train and test

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# splitting the encoded data
X_enc_train, X_enc_test, y_enc_train, y_enc_test = train_test_split(
    encoded_X, encoded_y, test_size=0.2
)

print(
    f"Encoded train data X: {X_enc_train.shape}, Y: {y_enc_train.shape}, X_test :{X_enc_test.shape}, Y_test: {y_enc_test.shape}"
)
print(
    f"Actual train & test data X: {X_train.shape}, Y: {X_train.shape}, X_test :{X_test.shape}, Y_test: {y_test.shape}"
)

"""**Support Vector Classifier**"""

# applying support vector classifier on the encoded data

svc_clf = SVC(probability=True)

svc_clf.fit(X_enc_train, y_enc_train)
svc_predictions = svc_clf.predict(X_enc_test)
y_proba = svc_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy of Support Vector Classifierr {:.2f}".format(accuracy_score(y_enc_test, svc_predictions)))
print("                                                                 ")

print(f"ROC-AUC Score of Support Vector Classifier: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                                  ")

print(f"Average Precision-Recall Score of Support Vector Classifier: {average_precision:.2f}")
print("                                                                 ")

print(
    "\nSupport Vector Classifier Classification report \n {0}".format(
        classification_report(y_enc_test, svc_predictions)
    )

)

conf_matrix=confusion_matrix(svc_predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, svc_predictions) , '\n')

plt.title("Support Vector Classifier - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

"""**RandomForestClassifier**"""

# applying RandomForestClassifier on the encoded data

rf_clf = RandomForestClassifier()

rf_clf.fit(X_enc_train, y_enc_train)
predictions = rf_clf.predict(X_enc_test)
y_proba = rf_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy score of Random Forest : {:.2f}".format(accuracy_score(y_enc_test, predictions)))
print("                                                          ")

print(f"ROC-AUC Score of Random Forest: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                           ")

print(f"Average Precision-Recall Score: {average_precision:.2f}")
print("                                                            ")

print(
    "\nRandom Forest Classification report \n {0}".format(
        classification_report(y_enc_test, predictions)
    )
)

conf_matrix=confusion_matrix(predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, predictions) , '\n')

plt.title("Random Forest Classification - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

"""**Logistic Regression**"""

# applying logistic regression model on the encoded data

lr_clf = LogisticRegression()

lr_clf.fit(X_enc_train, y_enc_train)
lr_predictions = lr_clf.predict(X_enc_test)
y_proba = lr_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy score of logistic regression : {:.2f}".format(accuracy_score(y_enc_test, lr_predictions)))
print("                                                          ")

print(f"ROC-AUC Score of logistic regression: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                           ")

print(f"Average Precision-Recall Score of logistic regression: {average_precision:.2f}")
print("                                                            ")

print(
    "\nlogistic regression Classification report \n {0}".format(
        classification_report(y_enc_test, lr_predictions)


    )
)
conf_matrix=confusion_matrix(lr_predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, lr_predictions) , '\n')

plt.title("logistic regression - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

"""**LGBM Classifier**"""

# Applying LGBMClassifier model on the encoded data
lgbm_clf = LGBMClassifier()

lgbm_clf.fit(X_enc_train, y_enc_train)
lgbm_predictions = lgbm_clf.predict(X_enc_test)
y_proba = lgbm_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy score of LGBMClassifier : {:.2f}".format(accuracy_score(y_enc_test, lgbm_predictions)))
print("                                                          ")

print(f"ROC-AUC Score of LGBMClassifier: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                           ")

print(f"Average Precision-Recall Score of LGBMClassifier: {average_precision:.2f}")
print("                                                            ")

print(
    "\nLGBMClassifier Classification report \n {0}".format(
        classification_report(y_enc_test, lgbm_predictions)
    )
)

conf_matrix=confusion_matrix(lgbm_predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, lgbm_predictions) , '\n')

plt.title("LGBM Classifier  - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

"""**AdaBoost Classifier**"""

# Applying AdaBoostClassifier model on the encoded data

adaboost_clf = AdaBoostClassifier(base_estimator=LogisticRegression(), n_estimators=50, random_state=42)

adaboost_clf.fit(X_enc_train, y_enc_train)
adaboost_predictions = adaboost_clf.predict(X_enc_test)
y_proba = adaboost_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy score of AdaBoost Classifier: {:.2f}".format(accuracy_score(y_enc_test, adaboost_predictions)))
print("                                                          ")

print(f"ROC-AUC Score of AdaBoost Classifier: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                           ")

print(f"Average Precision-Recall Score of AdaBoost Classifier: {average_precision:.2f}")
print("                                                            ")

print(
    "\nAdaBoost Classifier Classification report \n {0}".format(
        classification_report(y_enc_test, adaboost_predictions)
    )
)

conf_matrix=confusion_matrix(adaboost_predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, adaboost_predictions) , '\n')

plt.title("AdaBoost Classifier - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

"""**XGB Classifier**"""

# Applying XGBClassifier model on the encoded data

xgb_clf = XGBClassifier()

xgb_clf.fit(X_enc_train, y_enc_train)
xgb_predictions = xgb_clf.predict(X_enc_test)
y_proba = xgb_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy score of XGB Classifier: {:.2f}".format(accuracy_score(y_enc_test, xgb_predictions)))
print("                                                          ")

print(f"ROC-AUC Score of XGB Classifier: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                           ")

print(f"Average Precision-Recall Score of XGB Classifier: {average_precision:.2f}")
print("                                                            ")

print(
    "\nXGB Classifier Classification report \n {0}".format(
        classification_report(y_enc_test, xgb_predictions)
    )
)

conf_matrix=confusion_matrix(xgb_predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, xgb_predictions) , '\n')

plt.title("XGB Classifier - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()

"""**CatBoost Classifier**"""

# Applying CatBoostClassifier model on the encoded data

catboost_clf = CatBoostClassifier()

catboost_clf.fit(X_enc_train, y_enc_train)
catboost_predictions = catboost_clf.predict(X_enc_test)
y_proba = catboost_clf.predict_proba(X_enc_test)[:, 1]
average_precision = average_precision_score(y_enc_test, y_proba)

print("Accuracy score of CatBoost Classifier : {:.2f}".format(accuracy_score(y_enc_test, catboost_predictions)))
print("                                                          ")

print(f"ROC-AUC Score of CatBoost Classifier: {roc_auc_score(y_enc_test, y_proba):.2f}")
print("                                                           ")

print(f"Average Precision-Recall Score of CatBoost Classifier: {average_precision:.2f}")
print("                                                            ")

print(
    "\nCatBoost Classifier Classification report \n {0}".format(
        classification_report(y_enc_test, catboost_predictions)
    )
)

conf_matrix=confusion_matrix(catboost_predictions,y_enc_test)
labels= ['Non_Suspicious_Fraud', 'Suspicious_Fraud']
plt.figure(figsize=(6, 6))

sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels,
            linewidths= 0.05 ,annot=True, fmt="d" , cmap='BuPu')

print(classification_report(y_enc_test, catboost_predictions) , '\n')

plt.title("CatBoost Classifierr - Confusion Matrix")
plt.ylabel('True Value')
plt.xlabel('Predicted Value')
plt.show()